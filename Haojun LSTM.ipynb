{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Style Transfer through LSTM\n",
    "\n",
    "Here we are attempting to see whether we can transfer the style of the music by changing the notes themselves. Here, our definition of style transfer has changed. We are interested in seeing whether a network can generate music by being \"inspired\" from another music.\n",
    "\n",
    "To do this, we will first train a CNN that classifies music well. Then, we will use the actiavtions from one of the layers as a feature extractor and extract features from music. \n",
    "\n",
    "Next step is to use the features we extracted to generate the music back. This is done by conditioning a (possibly bidirectional) LSTM using the feature vector, and then train it to generate the music that was used to extract the feature vector. We will train 2 such generators.\n",
    "\n",
    "Last step is to see whether the generators works as a translator. We will take one music from one class and extract the features. And then feed the feature into the other class to generate music. We will see if the network picks up the styles and whether the network can be \"inspired\" by the music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mido\n",
    "import keras\n",
    "import numpy as np\n",
    "import sklearn.model_selection as ms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('matricies/notes.npy')\n",
    "Y = np.load('matricies/labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_small = np.load('matricies/small_notes.npy')\n",
    "Y_small = np.load('matricies/small_Y.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task we will toss the first dimension since it does not add additional information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = X[:,:,:,1]\n",
    "X_small = X_small[:,:,:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (DO NOT RUN THIS) CNN encoding\n",
    "\n",
    "In this section we first try to train a CNN that classifies music well. Here we will use a simple structure of\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = ms.train_test_split(X, Y, test_size=0.1, random_state=123)\n",
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1(input_shape):\n",
    "    X_input = keras.layers.Input(input_shape)\n",
    "    X = X_input\n",
    "    X = keras.layers.Conv1D(filters=4, kernel_size=32, padding='same', name='Conv0',\n",
    "                            dilation_rate=4,\n",
    "                            kernel_initializer=keras.initializers.glorot_normal(seed=None),\n",
    "                            bias_initializer=keras.initializers.glorot_normal(seed=None),\n",
    "                            data_format=\"channels_last\")(X)\n",
    "    X = keras.layers.AveragePooling1D(4)(X)\n",
    "    X = keras.layers.BatchNormalization(axis = 2, name = 'bn0')(X)\n",
    "    X = keras.layers.Activation('relu')(X)\n",
    "    print(X.shape)\n",
    "    \n",
    "#     X = keras.layers.Reshape((250, -1))(X)\n",
    "    X = keras.layers.Conv1D(filters=16, kernel_size=16, padding='valid', name='Conv1',\n",
    "                            dilation_rate=2,\n",
    "                            kernel_initializer=keras.initializers.glorot_normal(seed=None),\n",
    "                            bias_initializer=keras.initializers.glorot_normal(seed=None),\n",
    "                            data_format=\"channels_last\")(X)\n",
    "    X = keras.layers.AveragePooling1D(2)(X)\n",
    "    X = keras.layers.BatchNormalization(axis = 2, name = 'bn1')(X)\n",
    "    X = keras.layers.Activation('relu')(X)\n",
    "    print(X.shape)\n",
    "    X = keras.layers.Flatten()(X)\n",
    "#     X = keras.layers.Dense(100, activation='sigmoid')(X)\n",
    "#     X = keras.layers.Dropout(0.5)(X)    \n",
    "    X = keras.layers.Dense(50, activation='sigmoid')(X)\n",
    "    X = keras.layers.Dropout(0.5)(X)    \n",
    "#     X = keras.layers.Dense(10, activation='sigmoid')(X)\n",
    "    X = keras.layers.Dense(1, activation='sigmoid')(X)\n",
    "    model = keras.models.Model(inputs=X_input, outputs=X, name='basic')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model1(X.shape[1:])\n",
    "m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fit(X_train, Y_train, epochs=4, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = m.evaluate(X_test, Y_test, batch_size=32)\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save('cnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (DO NOT RUN THIS SECTION) Feature Extraction\n",
    "\n",
    "Below is code for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = keras.models.load_model('cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_features(X, model, layer_nums, batch_num):\n",
    "    \"\"\"Extract all the features by taking out the activation output of the layers specified in layer_nums\n",
    "    \n",
    "        X : the data that we are trying to extract features from\n",
    "        model: the model that we are using to extract the features\n",
    "        layer_nums: the layer number that we want to use as feature extractors\n",
    "    \"\"\"\n",
    "    outputs = [model.layers[layer].output for layer in layer_nums]\n",
    "    f = keras.backend.function([model.input, keras.backend.learning_phase()], outputs)\n",
    "    ## split up into smaller chunks\n",
    "    layer_outs = []\n",
    "    m = X.shape[0]\n",
    "    for i in range(batch_num + 1):\n",
    "        layer_outs += [f([X[(m // batch_num * i):(m // batch_num * (i+1)), :, :], 0.])]\n",
    "    result = []\n",
    "    for i in range(len(layer_nums)):\n",
    "        layer_outs2 = []\n",
    "        for batch in range(batch_num):\n",
    "            activations = layer_outs[batch][i]\n",
    "            activations = activations.reshape(activations.shape[0], -1)\n",
    "            layer_outs2 += [activations]\n",
    "        result += [np.vstack(np.array(layer_outs2))]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classical_features = extract_all_features(X_small[(Y_small == 1).ravel(), :, :], m, [9, 10], 160)\n",
    "jazz_features = extract_all_features(X_small[(Y_small == 0).ravel(), :, :], m, [9, 10], 160)\n",
    "np.savetxt('matricies/classical_features_big.txt', classical_features[0])\n",
    "np.savetxt('matricies/jazz_features_big.txt', jazz_features[0])\n",
    "np.savetxt('matricies/classical_features_small.txt', classical_features[1])\n",
    "np.savetxt('matricies/jazz_features_small.txt', jazz_features[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (RUN THIS SECTION)\n",
    "\n",
    "Here we will train 2 (bidirectional) LSTMs. One for generating classical music and one for generating jazz music. The way we condition the LSTMs is to have a Dense layer that transforms our feature vectors into the shape of the cell state of the LSTM, and then we will use this cell state as the initial cell state of the LSTM, and train it to generate the original MIDI matrix. \n",
    "\n",
    "The input to the model is a feature vector. It will then have a dense layer that transforms it into the cell state shape. Then for every time stamp it will generate some music, and the output should be a music.\n",
    "\n",
    "First we need to construct the new X and Ys. The X will be the same but with the last layer stacked on top of each other insead. Y will be the same as X but all values shifted left by 1 timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classical_features_big = np.loadtxt('matricies/classical_features_big.txt')\n",
    "jazz_features_big = np.loadtxt('matricies/jazz_features_big.txt')\n",
    "classical_features_small = np.loadtxt('matricies/classical_features_small.txt')\n",
    "jazz_features_small = np.loadtxt('matricies/jazz_features_small.txt')\n",
    "classical_features = [classical_features_big, classical_features_small]\n",
    "jazz_features = [jazz_features_big, jazz_features_small]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_T = 480\n",
    "m_ckeep = classical_features[1].shape[0]\n",
    "m_jkeep = jazz_features[1].shape[0]\n",
    "X2_classical = X_small[(Y_small==1).ravel(), :,:][:m_ckeep,:gen_T,:]\n",
    "X2_jazz = X_small[(Y_small==0).ravel(), :m_jkeep,:][:m_jkeep,:gen_T,:]\n",
    "\n",
    "Y2_classical = np.zeros(X2_classical.shape, dtype=np.int32)\n",
    "Y2_classical[:-1,:,:] = X2_classical[1:,:,:]\n",
    "Y2_jazz = np.zeros(X2_jazz.shape, dtype=np.int32)\n",
    "Y2_jazz[:-1,:,:] = X2_jazz[1:,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is inspired and borrowed from Coursera Sequence Model Module/Improvise a Jazz Solo with an LSTM Network - v3 programming assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 64\n",
    "n_values = 59\n",
    "LSTM_cell = keras.layers.LSTM(n_a, return_state = True)\n",
    "feature_densor = keras.layers.Dense(n_a, activation='sigmoid')\n",
    "# on_off_densor = keras.layers.Dense(n_values, activation='sigmoid')\n",
    "sustain_densor = keras.layers.Dense(n_values, activation='sigmoid')\n",
    "reshapor = keras.layers.Reshape((1, n_values))\n",
    "concatenator = keras.layers.Concatenate(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(feature_shape, Tx, n_channel):\n",
    "    feature_input = keras.layers.Input(feature_shape, name='feature_inp')\n",
    "    music_input = keras.layers.Input((Tx, n_channel), name='X')\n",
    "    a0 = keras.layers.Input((n_a,), name='a0')\n",
    "    \n",
    "#     feature = keras.layers.Flatten()(feature_input)\n",
    "    c0 = feature_densor(feature_input)\n",
    "    X = music_input\n",
    "    a = a0\n",
    "    c = c0\n",
    "    outputs = []\n",
    "    for t in range(Tx):\n",
    "        if t % 200 == 0:\n",
    "            print(t)\n",
    "        x = keras.layers.Lambda(lambda x: X[:, t, :])(X)\n",
    "        x = reshapor(x)\n",
    "        a, _, c = LSTM_cell(x, initial_state=[a, c])\n",
    "#         on_off_out = on_off_densor(a)\n",
    "        sustain_out = sustain_densor(a)\n",
    "#         outputs += [concatenator([on_off_out, sustain_out])]\n",
    "        outputs += [sustain_out]\n",
    "    return keras.models.Model(inputs=[feature_input, music_input, a0], outputs=outputs)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "200\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "lstm_model = lstm((classical_features[1].shape[1], ), gen_T, X.shape[2])\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.load_weights('lstm_classical.h55')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "mini_size = X2_classical.shape[0]\n",
    "a0 = np.zeros((mini_size, n_a))\n",
    "lstm_model.fit([classical_features[1][:mini_size:,:], X2_classical[:mini_size,:,:], a0],\n",
    "               list(Y2_classical[:mini_size,:,:].transpose((1, 0, 2))), epochs=500);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.save_weights('lstm_classical.h55')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Music with LSTM\n",
    "\n",
    "Now we will sample the LSTM and densor we just trained to generate some cool music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_one_hot(x):\n",
    "    random_tensor = tf.random_uniform(tf.shape(x), 0.03, 1.0)\n",
    "    return tf.to_float(tf.greater(x, random_tensor))\n",
    "one_hot = keras.layers.Lambda(set_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_generate(LSTM_cell, sustain_densor, feature_shape, n_values, n_a, Ty=200):\n",
    "    x0 = keras.layers.Input((1, n_values), name='x0')\n",
    "    feature_input = keras.layers.Input(feature_shape, name='feature_inp')\n",
    "    a0 = keras.layers.Input((n_a,), name='a0')\n",
    "    \n",
    "    c0 = feature_densor(feature_input)\n",
    "    a = a0\n",
    "    c = c0\n",
    "    x = x0\n",
    "    outputs = []\n",
    "    for t in range(Ty):\n",
    "        a, _, c = LSTM_cell(x, initial_state=[a, c])\n",
    "        out = sustain_densor(a)\n",
    "        out = one_hot(out)\n",
    "        out = reshapor(out)\n",
    "        outputs += [out]\n",
    "        x = out\n",
    "    return keras.models.Model(inputs=[feature_input, x0, a0], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_model = lstm_generate(LSTM_cell, sustain_densor, (classical_features[1].shape[1],), n_values, n_a, Ty=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50) (1, 1, 59) (1, 64)\n"
     ]
    }
   ],
   "source": [
    "feature_input = classical_features[1][:1,:]\n",
    "x0 = np.zeros((1, 1, n_values))\n",
    "a0 = np.zeros((1, n_a,))\n",
    "print(feature_input.shape, x0.shape,a0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = generate_model.predict([feature_input, x0, a0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 59)\n"
     ]
    }
   ],
   "source": [
    "song = np.array(pred).reshape(-1, n_values)\n",
    "# song = song > 0.5\n",
    "print(song.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15a2b37d9b0>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ticks_per_beat = 8\n",
    "notes_to_keep_down = 32\n",
    "def generate(sustain_mat, velocity_mat):\n",
    "    new_midi = mido.MidiFile(type=0)\n",
    "    new_midi.ticks_per_beat = target_ticks_per_beat\n",
    "    track = mido.MidiTrack()\n",
    "    new_midi.tracks.append(track)\n",
    "\n",
    "    track.append(mido.MetaMessage('set_tempo', tempo=500000, time=0))\n",
    "    track.append(mido.MetaMessage('time_signature', numerator=4, denominator=4, \n",
    "                             clocks_per_click=24, notated_32nd_notes_per_beat=8, time=0))\n",
    "    track.append(mido.MetaMessage('track_name', name='Test Track', time=0))\n",
    "\n",
    "    T, N = sustain_mat.shape\n",
    "    notes_on = [False] * N\n",
    "    prev_event_t = 0\n",
    "    for t in range(T):\n",
    "        for n in range(N-2):\n",
    "            if sustain_mat[t, n] > 0 and not notes_on[n]:\n",
    "                velocity = velocity_mat[t, n]\n",
    "                track.append(mido.Message('note_on', note=n+notes_to_keep_down, velocity=velocity, time=t-prev_event_t))\n",
    "                prev_event_t = t\n",
    "                notes_on[n] = True\n",
    "            if sustain_mat[t, n] == 0 and notes_on[n]:\n",
    "                track.append(mido.Message('note_on', note=n+notes_to_keep_down, velocity=0, time=t-prev_event_t))\n",
    "                prev_event_t = t\n",
    "                notes_on[n] = False\n",
    "        if sustain_mat[t, N-2] == 1 and not notes_on[N-2]:\n",
    "            track.append(mido.Message('control_change', control=64, value=127, time=t-prev_event_t))\n",
    "            prev_event_t = t\n",
    "            notes_on[N-2] = True\n",
    "        if sustain_mat[t, N-1] == 1 and not notes_on[N-1]:\n",
    "            track.append(mido.Message('control_change', control=67, value=127, time=t-prev_event_t))\n",
    "            prev_event_t = t\n",
    "            notes_on[N-1] = True\n",
    "        if notes_on[N-2] and sustain_mat[t, N-2] == 0:\n",
    "            track.append(mido.Message('control_change', control=64, value=0, time=t-prev_event_t))\n",
    "            prev_event_t = t\n",
    "            notes_on[N-2] = False\n",
    "        if notes_on[N-1] and sustain_mat[t, N-1] == 0:\n",
    "            track.append(mido.Message('control_change', control=67, value=0, time=t-prev_event_t))\n",
    "            prev_event_t = t\n",
    "            notes_on[N-1] = False\n",
    "    return new_midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_midi = generate((song > 0.02).reshape(200, 130,2) , np.ones(song.shape, dtype=np.int8) * 64, info)\n",
    "# new_midi = X2_classical[100,:,:]\n",
    "new_midi = song\n",
    "new_midi = generate(new_midi, np.ones(new_midi.shape, dtype=np.int8) * 127)\n",
    "# plt.imshow(new_midi)\n",
    "new_midi.save('temp.midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
